{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #2590c2; text-align: center;\">\n",
    "<span style=\"font-size:18pt;\"><b>ST: DEEP LEARNING</b></span><br/>\n",
    "<span>(CS 696-04) (SM18)</span><br/><br/>\n",
    "<span><b>Homework 1</b></span><br/><br/>\n",
    "<span>Submitted By</span><br/>\n",
    "<span>Ashok Kumar Shrestha</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><u>Tasks:</u></h4><br/>\n",
    "<span> \n",
    "In this assignment, you are asked to implement a deep fully-connected MLP with three hidden layers. \n",
    "Your program is supposed to be working on both the MNIST data and CIFAR-10 data.  \n",
    "Both of them are 10-class classification problem.  When implementing the program, please take the \n",
    "following into consideration: </span>\n",
    "    <ul>\n",
    "    <li>\n",
    "Like any machine learning assignment, you need to divide your dataset into training and testing. \n",
    "</li><li>\n",
    "The flexibility of the program: an ideal program should allow the following: number of layers are not \n",
    "hard-coded, different activation functions can be used, etc. In other words, different combination can be \n",
    "easily built on top of the modules of your program. \n",
    "</li><li>\n",
    "A detailed project report containing:  design of your program; flexibility of your program if any; the effect of using different learning rate; the plot of loss versus epoch; the plot of accuracy versus epoch\n",
    "Implement your program using Jupyter notebook. \n",
    "</li><li>\n",
    "Hard copy of report submitted in class on June 27th.  \n",
    "</li><li>\n",
    "The program is zipped into a single YourfistnameYourLastName.zip file and submit it online before class starts that day.\n",
    "</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Code:\n",
    "</b><br/>\n",
    "<span>\n",
    "Read data sets (MNIST and CIFAR-10) from the file.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read Data sets: MNIST and CIFAR-10\n",
    "-----------------------------------------------\n",
    "Parameters:\n",
    "===========\n",
    "file_name: file name to read\n",
    "\n",
    "Return:\n",
    "=======\n",
    "train_img, test_img, train_lbl, test_lbl values\n",
    "\"\"\"\n",
    "\n",
    "import cloudpickle as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000):\n",
    "    '''\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the neural net classifier.\n",
    "    '''\n",
    "    # Load the raw CIFAR-10 data\n",
    "    X_train, y_train, X_test, y_test = load(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    X_train = X_train.astype(np.float64)\n",
    "    X_val = X_val.astype(np.float64)\n",
    "    X_test = X_test.astype(np.float64)\n",
    "\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2)\n",
    "    X_val = X_val.transpose(0, 3, 1, 2)\n",
    "    X_test = X_test.transpose(0, 3, 1, 2)\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train)\n",
    "\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    X_train /= std\n",
    "    X_val /= std\n",
    "    X_test /= std\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'mean': mean_image, 'std': std\n",
    "    }\n",
    "\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    ''' load single batch of cifar '''\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding ='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "def get_CIFAR10(ROOT):\n",
    "    ''' load all of cifar '''\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Xte, Ytr, Yte\n",
    "\n",
    "def load_mnist(data_file=\"mnist.data\", test_size=0.10, random_state=0):\n",
    "    mnist = pickle.load(open(data_file, \"rb\"))\n",
    "    return train_test_split(mnist['data'], mnist['target'], test_size=test_size,\n",
    "                            random_state=random_state)\n",
    "\n",
    "def load(file_name):\n",
    "    if file_name == \"mnist\":\n",
    "        print(\"MNIST data loaded.\")\n",
    "        return load_mnist(data_file=\"mnist.data\", test_size=0.2, random_state=42)\n",
    "    \n",
    "    elif file_name == \"cifar\":\n",
    "        print(\"CIFAR data loaded.\")\n",
    "        train_img, test_img, train_lbl, test_lbl = get_CIFAR10(\"\")\n",
    "    \n",
    "        # covert N x 3 x 32 x 32 to N x 3072\n",
    "        '''\n",
    "        train_img = np.reshape(train_img, (len(train_img), 3 * 32 * 32))\n",
    "        test_img = np.reshape(test_img, (len(test_img), 3 * 32 * 32))\n",
    "        '''\n",
    "        train_img = np.reshape(train_img, (-1, 3072))\n",
    "        test_img = np.reshape(test_img, (-1, 3072))\n",
    "        \n",
    "        return train_img, test_img, train_lbl, test_lbl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Main Program:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Network...\n",
      "-------------------------------------------------------\n",
      "Reading Data sets...\n",
      "MNIST data loaded.\n",
      "-------------------------------------------------------\n",
      "Training started...\n",
      "Iteration: 0 | Loss: 2.6790060177670245\n",
      "Iteration: 10 | Loss: 0.4297826865517941\n",
      "Iteration: 20 | Loss: 0.2643785269717866\n",
      "Iteration: 30 | Loss: 0.09177066380685146\n",
      "Iteration: 40 | Loss: 0.14856892016033088\n",
      "Iteration: 50 | Loss: 0.08807304070928765\n",
      "Iteration: 60 | Loss: 0.05594001047213199\n",
      "Iteration: 70 | Loss: 0.10461389101180359\n",
      "Iteration: 80 | Loss: 0.07189120708043555\n",
      "Iteration: 90 | Loss: 0.06609034207967879\n",
      "Training ended.\n",
      "-------------------------------------------------------\n",
      "Testing started...\n",
      "Testing ended.\n",
      "-------------------------------------------------------\n",
      "Training Accuracy: 99.18 %\n",
      "Test Accuracy: 97.16 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0783b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code: Main program to execute. Users can also run test.py for the same.\n",
    "-----------------------------------------------------------------------\n",
    "Features:\n",
    "* Customizable no. of Hidden layers (activation functions and no. of neurons/units)\n",
    "* Customizable Data sets (MNIST and CIFAR-10)\n",
    "* Visualization of Error vs Epochs\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ann_classifier import Network as Network\n",
    "\n",
    "def main():\n",
    "    print(\"Starting Network...\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Reading Data sets...\")\n",
    "    \n",
    "    # MNIST Data sets\n",
    "    train_img, test_img, train_lbl, test_lbl = load(file_name=\"mnist\")\n",
    "    \n",
    "    # CIFAR-10 Data sets\n",
    "    #train_img, test_img, train_lbl, test_lbl = load(file_name=\"cifar\")\n",
    "    \n",
    "    y = train_lbl[:].astype(int)\n",
    "    x = train_img[:]/255.\n",
    "    testY = test_lbl[:].astype(int)\n",
    "    testX = test_img[:]/255.\n",
    "    \n",
    "    n_in = x.shape[1]\n",
    "    n_out = 10\n",
    "    \n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Training started...\")\n",
    "    \n",
    "    nn = Network(n_in=n_in, n_out=n_out, l_rate=0.1)\n",
    "    nn.add_layer(activation_function=\"tanh\", n_neurons=64, is_dropout=True, drop_out=0.7)\n",
    "    nn.add_layer(activation_function=\"tanh\", n_neurons=32, is_dropout=False, drop_out=0.5)\n",
    "    nn.add_layer(activation_function=\"softmax\", n_neurons=n_out)\n",
    "    nn.train(x,y, n_epoch=100, print_loss=True, batch_size=64)\n",
    "    \n",
    "    print(\"Training ended.\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Testing started...\")\n",
    "\n",
    "    train_accuracy = nn.getAccuracy(x,y)\n",
    "    test_accuracy = nn.getAccuracy(testX,testY)\n",
    "\n",
    "    print(\"Testing ended.\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    \n",
    "    print('Training Accuracy: {0:0.2f} %'.format(train_accuracy))\n",
    "    print('Test Accuracy: {0:0.2f} %'.format(test_accuracy))\n",
    "    \n",
    "    nn.show_graph()\n",
    "    \n",
    "if __name__ ==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Implementation:\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "The projects consist of the following modules:\n",
    "<ul>\n",
    "<li>\n",
    "activations.py\n",
    "</li><li>\n",
    "ann_classifier.py\n",
    "</li><li>\n",
    "cost_functions.py\n",
    "</li><li>\n",
    "decay.py\n",
    "</li><li>\n",
    "layers.py\n",
    "</li><li>\n",
    "read_file.py\n",
    "</li><li>\n",
    "test.py\n",
    "</li><li>\n",
    "weight_initialization.py\n",
    "</ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "The project can be run by executing test.py file.  All the codes are zipped to \"AshokShrestha.zip\" zip file. In this zip file, datasets are not included for reducing the file size. Please copy the data sets (MNIST and CIFAR-10) into the folder containing the code.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b> Task 1:</b> &nbsp;\n",
    "<span>Divide dataset into training and testing</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "Separate module <b>\"read_file.py\"</b> has been created for the above mentioned task. Import this module from main program to\n",
    "read both MNIST data and CIFAR-10 data.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MNIST Data:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading MNIST Data sets\n",
    "train_img, test_img, train_lbl, test_lbl = load(file_name=\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CIFAR-10 Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CIFAR-10 Data sets\n",
    "train_img, test_img, train_lbl, test_lbl = load(file_name=\"cifar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task 2:</b> &nbsp;\n",
    "<span>Flexibility of the program</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "The separate module <b>\"ann_classifier.py\"</b> has been created to implement MLP with three hidden layers as per the task \n",
    "mentioned. Also user can easily add or remove hidden layer/s as required. Similarly, users can specify the desired activation\n",
    "functions listed in module <b>\"activations.py\"</b> along with the number of units in the hidden layer.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    nn = Network(n_in=n_in, n_out=n_out)\n",
    "    nn.add_layer(activation_function=\"tanh\", n_neurons=15)\n",
    "    nn.add_layer(activation_function=\"relu\", n_neurons=10)\n",
    "    nn.add_layer(activation_function=\"softmax\", n_neurons=n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "Here,<br/> \n",
    "n_in : number of input units in the network,<br/>\n",
    "n_out: number of ouput units in the network,<br/>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "New hidden layer can be added with <b>\"add_layer()\"</b> method. It takes two parameters: name of the activation function and\n",
    "the number of units in the hidden layer. Users can choose activation functions from following list.\n",
    "</span>\n",
    "<ol>\n",
    "<li>\n",
    "Sigmoid\n",
    "</li><li>\n",
    "Bipolar Sigmoid\n",
    "</li><li>\n",
    "Softmax\n",
    "</li><li>\n",
    "Tanh\n",
    "</li><li>\n",
    "Relu\n",
    "</li><li>\n",
    "Leaky Relu\n",
    "</ol>\n",
    "\n",
    "<span>\n",
    "Following code is the activation functions and their corresponding first derivative (prime) implented in\n",
    "<b>\"activations.py\"</b> module.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # z = sigmoid(x)\n",
    "    return z * (1 - z)\n",
    "\n",
    "\n",
    "def bipolar_sigmoid(z):\n",
    "    return -1.0 + 2.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def bipolar_sigmoid_prime(z):\n",
    "    # z = bipolar_sigmoid(x)\n",
    "    return (1.0 - np.square(z)) / 2.0\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1)).T\n",
    "    \n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "\n",
    "def tanh_prime(z):\n",
    "    # z = tanh(x)\n",
    "    return 1 - z * z\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "def relu_prime(z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z < 0] = 0\n",
    "    return dz\n",
    "\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(z, z * alpha)\n",
    "\n",
    "\n",
    "def leaky_relu_prime(z, alpha=0.01):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z < 0] = alpha\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can also specify different cost functions for the network specified in <b>\"cost_functions.py\"</b> module. \n",
    "The list includes:\n",
    "<ol>\n",
    "<li>\n",
    "Softmax Cost\n",
    "</li><li>\n",
    "Cross Entropy Cost\n",
    "</li><li>\n",
    "Linear Cost\n",
    "</li><li>\n",
    "Mean Squared Cost\n",
    "</li><li>\n",
    "Mean Squared Linalg Cost\n",
    "</li><li>\n",
    "Quadratic Cost\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax_cost(a, p):\n",
    "    correct_log_probs = -np.log(p[range(len(a)), a])\n",
    "    return np.sum(correct_log_probs)\n",
    "\n",
    "\n",
    "def cross_entropy_cost(a, p):\n",
    "    p = np.array(p).reshape(len(a), 1)\n",
    "    cost = - (a * np.log(p) + (1 - a) * np.log(1 - p))\n",
    "    return np.sum(cost)\n",
    "\n",
    "\n",
    "def linear_cost(a, p):\n",
    "    delta = a - np.array(p).reshape(len(a), 1)\n",
    "    return np.mean(delta)\n",
    "\n",
    "\n",
    "def mean_square(a, p):\n",
    "    delta = a - np.array(p).reshape(len(a), 1)\n",
    "    error = np.sum(np.square(delta))\n",
    "    return error / len(a)\n",
    "\n",
    "\n",
    "def mean_square_linalg(a, p):\n",
    "    delta = a - np.array(p).reshape(len(a), 1)\n",
    "    error = np.linalg.norm(delta)\n",
    "    return error / len(a)\n",
    "\n",
    "\n",
    "def quadratic_cost(a, p):\n",
    "    delta = a - np.array(p).reshape(len(a), 1)\n",
    "    cost = np.square(delta, axis=0)\n",
    "    return np.sum(cost) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "There is also option to use various weight initialization methods for each hidden layers. It is implemented in the\n",
    "<b>\"weight_initialization.py\"</b> module. List of available weight initialization methods are:\n",
    "<ol>\n",
    "<li>\n",
    "Xavier\n",
    "</li><li>\n",
    "HE\n",
    "</li><li>\n",
    "Interval (-0.5, 0.5)\n",
    "</li><li>\n",
    "Interval One (-1, 1)\n",
    "</li>\n",
    "</ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xavier(p, c):\n",
    "    return np.random.rand(p, c) / np.sqrt(p)\n",
    "\n",
    "\n",
    "def he(p, c):\n",
    "    return np.random.rand(p, c) * np.sqrt(2/(p + c))\n",
    "\n",
    "\n",
    "def interval(p, c):\n",
    "    return np.random.rand(p, c) - 0.5\n",
    "\n",
    "\n",
    "def interval_one(p, c):\n",
    "    return 2 * np.random.random((p, c)) - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "The separate module <b>\"decay.py\"</b> has been implemented to adapt the learning rate (alpha) with number of epochs.\n",
    "Users can choose decay from following list:\n",
    "<ol>\n",
    "<li>\n",
    "Step Decay\n",
    "</li><li>\n",
    "Exponential Decay\n",
    "</li>\n",
    "</ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch, initial_lrate=0.1, drop=0.6, epochs_drop=1000):\n",
    "    return initial_lrate * np.power(drop, np.floor((1 + epoch) / epochs_drop))\n",
    "\n",
    "\n",
    "def exp_decay(epoch, initial_lrate=0.1, k=0.1):\n",
    "    return initial_lrate * np.exp(-k * epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "In order to avoid local minima, momentum coeffiencient has been used for updating the weights. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "<b>Task 3 : </b>&nbsp;Report\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "For model visualization plot of cost of model vs the number of epochs is plotted.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Epochs, Cost, 'r--')\n",
    "plt.title(\"Cost vs Epoch\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Cost\")\n",
    "plt.savefig(\"cost_vs_epochs.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Learning rate:\n",
    "</b>\n",
    "<span>\n",
    "Testing with different learning rates.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "Testing the effect of different learning rates with following parameters.<br/><br/>\n",
    "Training size: 56,000<br/>\n",
    "Testing size:14,000<br/>\n",
    "Split size: 80% : 20%<br/>\n",
    "No. of hidden layers: 3<br/>\n",
    "Hidden layer 1: Tanh (17 units)<br/>\n",
    "Hidden layer 2: Tanh (10 units)<br/>\n",
    "Hidden layer 3: Softmax (10 units)<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "MNIST Data:\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mnist_lrate.png\"/>\n",
    "<img src=\"img/mnist_lrate_accuracy.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Accuracy vs Epoch\n",
    "</b>\n",
    "<br/>\n",
    "<span>\n",
    "Computing accuracy vs Epoch for the model with following parameters:<br/><br/>\n",
    "Training size: 56,000<br/>\n",
    "Testing size:14,000<br/>\n",
    "Split size: 80% : 20%<br/>\n",
    "No. of hidden layers: 3<br/>\n",
    "Hidden layer 1: Tanh (17 units)<br/>\n",
    "Hidden layer 2: Tanh (10 units)<br/>\n",
    "Hidden layer 3: Softmax (10 units)<br/>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "MNIST Data:\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mnist_epoch.png\"/>\n",
    "<img src=\"img/mnist_accuracy_epoch.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Accuracy:</b><br/><br/>\n",
    "<span>\n",
    "<u>MNIST Data:</u><br/>\n",
    "Training size: 56,000<br/>\n",
    "Testing size:14,000<br/>\n",
    "Split size: 80% : 20%<br/>\n",
    "No. of epochs: 200<br/>\n",
    "No. of hidden layers: 3<br/>\n",
    "Hidden layer 1: Tanh (17 units)<br/>\n",
    "Hidden layer 2: Tanh (10 units)<br/>\n",
    "Hidden layer 3: Softmax (10 units)<br/>\n",
    "Training Accuracy: 90.81 %<br/>\n",
    "Test Accuracy: 90.76 %\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mnist_cost_epochs.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "<u>CIFAR Data:</u><br/>\n",
    "Training size: 49,000<br/>\n",
    "Testing size:1,000<br/>\n",
    "No. of epochs: 500<br/>\n",
    "Accuracy: 35%<br/>\n",
    "No. of hidden layers: 3<br/>\n",
    "Hidden layer 1: Tanh (10 units)<br/>\n",
    "Hidden layer 2: Tanh (17 units)<br/>\n",
    "Hidden layer 3: Softmax (10 units)<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cifar_cost_epochs.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Testing the Code:\n",
    "</b><br/>\n",
    "<span>\n",
    "The main file to run is <b>\"test.py\"</b>. Open the file to modify/update necessary details such as adding hidden layers,\n",
    "testing with different activation functions, hidden layers neuron units, different weight initialization techniques,\n",
    "cost functions, learning rate decay and so on.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Note:\n",
    "</b>\n",
    "<span>\n",
    "All the codes are zipped to \"AshokShrestha.zip\" zip file. In this zip file, datasets are not included for reducing\n",
    "the file size. Please copy the data sets (MNIST and CIFAR-10) into the folder containing the code.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
