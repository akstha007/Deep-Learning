{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #2590c2; text-align: center;\">\n",
    "<span style=\"font-size:18pt;\"><b>ST: DEEP LEARNING</b></span><br/>\n",
    "<span>(CS 696-04) (SM18)</span><br/><br/>\n",
    "<span><b>Homework 2</b></span><br/><br/>\n",
    "<span>Submitted By</span><br/>\n",
    "<span>Ashok Kumar Shrestha</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><u>Tasks:</u></h4><br/>\n",
    "<ul><li>\n",
    "In this program assignment, you are asked to implement Convnet for Cifar-10 dataset. You need to implement conv layer, pooling layer and their backpropagation.   \n",
    "</li><li>\n",
    "When submitting your program, also include the data file in the zip file, so that it would be more convenient for me to test. Please also sprinkle your report into relevant places in the ipython notebook file. \n",
    "</li><li>\n",
    "In addition, submit a hard copy of your report on July 25th, 2018. Thanks. \n",
    "</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Code:\n",
    "</b><br/>\n",
    "<span>\n",
    "Read data sets (MNIST and CIFAR-10) from the file.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read Data sets: MNIST and CIFAR-10\n",
    "-----------------------------------------------\n",
    "Parameters:\n",
    "===========\n",
    "file_name: file name to read\n",
    "\n",
    "Return:\n",
    "=======\n",
    "train_img, test_img, train_lbl, test_lbl values\n",
    "\"\"\"\n",
    "import cloudpickle as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    X_train, y_train, X_test, y_test = load(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    X_train = X_train.astype(np.float64)\n",
    "    X_val = X_val.astype(np.float64)\n",
    "    X_test = X_test.astype(np.float64)\n",
    "\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2)\n",
    "    X_val = X_val.transpose(0, 3, 1, 2)\n",
    "    X_test = X_test.transpose(0, 3, 1, 2)\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train)\n",
    "\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    X_train /= std\n",
    "    X_val /= std\n",
    "    X_test /= std\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'mean': mean_image, 'std': std\n",
    "    }\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    ''' load single batch of cifar '''\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding ='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def get_CIFAR10(ROOT):\n",
    "    ''' load all of cifar '''\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Xte, Ytr, Yte\n",
    "\n",
    "def load_mnist(data_file=\"mnist.data\", test_size=0.10, random_state=0):\n",
    "    mnist = pickle.load(open(data_file, \"rb\"))\n",
    "    \n",
    "    mnist['data'] = np.reshape(mnist['data'],(mnist['data'].shape[0],1,28,28))\n",
    "    return train_test_split(mnist['data'], mnist['target'], test_size=test_size,\n",
    "                            random_state=random_state)\n",
    "\n",
    "def load(file_name):\n",
    "    if file_name == \"mnist\":\n",
    "        print(\"MNIST data loaded.\")\n",
    "        return load_mnist(data_file=\"mnist.data\", test_size=0.2, random_state=42)\n",
    "    \n",
    "    elif file_name == \"cifar\":\n",
    "        print(\"CIFAR data loaded.\")\n",
    "        return get_CIFAR10(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Main Program:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code: Main program to execute. Users can also run test.py for the same.\n",
    "-----------------------------------------------------------------------\n",
    "Features:\n",
    "* Customizable type and no. of layers\n",
    "* Customizable Data sets (MNIST and CIFAR-10)\n",
    "* Customizable batch size and dropout\n",
    "* Visualization of Error vs Epochs\n",
    "\"\"\"\n",
    "\n",
    "import cost_functions as cost\n",
    "import numpy as np\n",
    "from ConvNet import ConvNet\n",
    "from Pool import Pool\n",
    "from Flatten import Flatten\n",
    "from FCLayer import FCLayer\n",
    "from model import Model\n",
    "\n",
    "def preprocess_data(X):\n",
    "    return (X-np.mean(X,axis=0))/np.std(X)\n",
    "\n",
    "def main():\n",
    "    print(\"Starting Network...\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Reading Data sets...\")\n",
    "    \n",
    "    # MNIST Data sets\n",
    "    #train_img, test_img, train_lbl, test_lbl = load(file_name=\"mnist\")\n",
    "    \n",
    "    # CIFAR-10 Data sets\n",
    "    train_img, test_img, train_lbl, test_lbl = load(file_name=\"cifar\")\n",
    "    \n",
    "    Y = train_lbl[:].astype(int)\n",
    "    X = train_img[:]/255.\n",
    "    Y_test = test_lbl[:].astype(int)\n",
    "    X_test = test_img[:]/255.\n",
    "\n",
    "    #preprocess data\n",
    "    X = preprocess_data(X)\n",
    "    X_test = preprocess_data(X_test)\n",
    "    \n",
    "    #model\n",
    "    model = Model()\n",
    "\n",
    "    model.add(ConvNet(filter_size=(5,5),filter_no=6,zero_padding=0,stride=(1,1),activation=\"relu\"))\n",
    "    model.add(Pool(pool_size=(2,2),stride=(2,2),pool_type=\"max\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(FCLayer(activation=\"relu\",n_neurons=32,l_rate=0.001, is_drop_out=True, drop_out=0.7))\n",
    "    model.add(FCLayer(activation=\"softmax\",n_neurons=10,l_rate=0.001))\n",
    "\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"CNN Layers:\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    model.print_layers()\n",
    "\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Begin Training...\")\n",
    "    \n",
    "    model.train(X,Y,n_epochs=7, print_loss=True, batch_size=32)\n",
    "    \n",
    "    print(\"End Training.\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Begin Testing...\")\n",
    "\n",
    "    train_accuracy = model.test(X,Y)\n",
    "    test_accuracy = model.test(X_test,Y_test)\n",
    "\n",
    "    print(\"End Testing.\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "\n",
    "    print('Training Accuracy: {0:0.2f} %'.format(train_accuracy))\n",
    "    print('Test Accuracy: {0:0.2f} %'.format(test_accuracy))\n",
    "    model.show_graph()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Implementation:\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>The projects consist of the following modules:</span>\n",
    "<ul>\n",
    "<li>\n",
    "activations.py\n",
    "</li><li>\n",
    "cost_functions.py\n",
    "</li><li>\n",
    "ConvNet.py\n",
    "</li><li>\n",
    "decay.py\n",
    "</li><li>\n",
    "FCLayer.py\n",
    "</li><li>\n",
    "Flatten.py\n",
    "</li><li>\n",
    "Layers.py\n",
    "</li><li>\n",
    "model.py\n",
    "</li><li>\n",
    "Pool.py\n",
    "</li><li>\n",
    "read_file.py\n",
    "</li><li>\n",
    "test.py\n",
    "</li><li>\n",
    "weight_initialization.py\n",
    "</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>The project can be run by executing test.py file.  All the codes are zipped to \"AshokShrestha.zip\" zip file.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b> Task 1:</b> &nbsp;\n",
    "<span>Divide dataset into training and testing</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Separate module <b>\"read_file.py\"</b> has been created for the above mentioned task. Import this module from main program to read both MNIST data and CIFAR-10 data.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MNIST Data:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading MNIST Data sets\n",
    "train_img, test_img, train_lbl, test_lbl = load(file_name=\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CIFAR-10 Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CIFAR-10 Data sets\n",
    "train_img, test_img, train_lbl, test_lbl = load(file_name=\"cifar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task 2:</b> &nbsp;\n",
    "<span>Flexibility of the program</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Creating the Convolution Neural Network model:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Adding ConvNet layer to model:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(ConvNet(filter_size=(5,5),filter_no=6,zero_padding=0,stride=(1,1),activation=\"relu\",l_rate=0.1))\n",
    "model.add(ConvNet(filter_size=(5,5),filter_no=16,zero_padding=0,stride=(1,1),activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Adding Pooling layer to model:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Pool(pool_size=(2,2),zero_padding=0,stride=(2,2),pool_type=\"max\"))\n",
    "model.add(Pool(pool_size=(2,2),zero_padding=0,stride=(2,2),pool_type=\"max\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Adding Flatten layer to model:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Adding fully connected layer to model:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(FCLayer(activation=\"relu\",n_neurons=120,l_rate=0.01, is_drop_out=True, drop_out=0.5))\n",
    "model.add(FCLayer(activation=\"relu\",n_neurons=84,l_rate=0.01, is_drop_out=True, drop_out=0.5))\n",
    "model.add(FCLayer(activation=\"softmax\",n_neurons=10,l_rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Traing the model:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X,Y,n_epochs=10, print_loss=True, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Testing the model:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = model.test(X,Y)\n",
    "test_accuracy = model.test(X_test,Y_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "Users can choose activation functions from following list.\n",
    "</span>\n",
    "<ol>\n",
    "    <li>\n",
    "    Sigmoid\n",
    "    </li><li>\n",
    "    Softmax\n",
    "    </li><li>\n",
    "    Tanh\n",
    "    </li><li>\n",
    "    Relu\n",
    "    </li><li>\n",
    "    Leaky Relu\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<span>\n",
    "Following code is the activation functions and their corresponding first derivative (prime) implented in\n",
    "<b>\"activations.py\"</b> module. For this project Relu is used as default.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters:\n",
    "===========\n",
    "z: input\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    return (np.exp(z).T / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_prime(z):\n",
    "    return 1 - z * z\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def relu_prime(z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z < 0] = 0\n",
    "    return dz\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(z, z * alpha)\n",
    "\n",
    "def leaky_relu_prime(z, alpha=0.01):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z < 0] = alpha\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can also specify different cost functions for the network specified in <b>\"cost_functions.py\"</b> module. \n",
    "The list includes:\n",
    "<ol>\n",
    "<li>\n",
    "Cross Entropy Cost\n",
    "</li><li>\n",
    "Linear Cost\n",
    "</li><li>\n",
    "Mean Squared Cost\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters:\n",
    "===========\n",
    "a: actual output\n",
    "p: predicted output\n",
    "'''\n",
    "def cross_entropy_cost(a, p):\n",
    "    m = len(a)\n",
    "    cost =  (-1 / m) * np.sum(a * np.log(p) + (1 - a) * np.log(1 - p))\n",
    "    return cost\n",
    "\n",
    "def linear_cost(a, p):\n",
    "    delta = a - np.array(p).reshape(len(a), 1)\n",
    "    return np.mean(delta)\n",
    "\n",
    "def mean_square(a, p):\n",
    "    delta = a - np.array(p).reshape(len(a), 1)\n",
    "    error = np.sum(np.square(delta))\n",
    "    return error / len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>There is also option to use various weight initialization methods for each hidden layers. It is implemented in the <b>\"weight_initialization.py\"</b> module. List of available weight initialization methods are:</span>\n",
    "<ol>\n",
    "<li>\n",
    "Xavier\n",
    "</li><li>\n",
    "HE\n",
    "</li><li>\n",
    "Other variation of HE\n",
    "</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters:\n",
    "===========\n",
    "p: no. of neurons in previous layer (l-1)\n",
    "c: no. of neurons in current layer (l)\n",
    "'''    \n",
    "def he(p, c):\n",
    "    return np.random.rand(p, c) * np.sqrt(2/p)\n",
    "\n",
    "def xavier(p, c):\n",
    "    return np.random.rand(p, c) * np.sqrt(1/p)\n",
    "\n",
    "def _he(p, c):\n",
    "    return np.random.rand(p, c) * np.sqrt(2/ (p + c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "The separate module <b>\"decay.py\"</b> has been implemented to adapt the learning rate (alpha) with number of epochs.\n",
    "Users can choose decay from following list:\n",
    "<ol>\n",
    "<li>\n",
    "Step Decay\n",
    "</li><li>\n",
    "Exponential Decay\n",
    "</li>\n",
    "</ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch, initial_lrate=0.1, drop=0.6, epochs_drop=1000):\n",
    "    return initial_lrate * np.power(drop, np.floor((1 + epoch) / epochs_drop))\n",
    "\n",
    "def exp_decay(epoch, initial_lrate=0.1, k=0.1):\n",
    "    return initial_lrate * np.exp(-k * epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "In order to avoid local minima, momentum coeffiencient has been used for updating the weights. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Used Adam for gradient descent optimization.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #Adam optimization\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        eps = 1e-8\n",
    "        t = self.n_epochs\n",
    "        m = self.m\n",
    "        v = self.v\n",
    "        \n",
    "        m = beta1*m + (1-beta1)*dw\n",
    "        mt = m / (1-beta1**t)\n",
    "        \n",
    "        v = beta2*v + (1-beta2)*(dw**2)\n",
    "        vt = v / (1-beta2**t)\n",
    "        \n",
    "        self.weights += self.l_rate * mt / (np.sqrt(vt) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "<b>Task 3 : </b>&nbsp;Report\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>For model visualization plot of cost of model vs the number of epochs is plotted.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Epochs, Cost, 'r--')\n",
    "plt.title(\"Cost vs Epoch\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Cost\")\n",
    "plt.savefig(\"cost_vs_epochs.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Learning rate:</b>\n",
    "<span>Testing with different learning rates.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>\n",
    "    <b><u>CIFAR-10 data:</u></b><br/>\n",
    "No. of epochs: 150<br/>\n",
    "Accuracy: 58.3%<br/>\n",
    "    ConvNet: filter(5X5), stride(1X1), padding:0, \"leaky_relu\"<br/>\n",
    "    Maxpooling: filter(5X5), stride(1X1)<br/>\n",
    "    Flatten:\n",
    "    FCLayer: neurons:32, l_rate:0.0001, \"leaky_relu\"<br/>\n",
    "    FCLayer: neurons:10, l_rate:0.0001, \"softmax\"<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/error_vs_epoch.png\" style=\"height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
